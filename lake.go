package lake

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/hkloudou/lake/v2/internal/cache"
	"github.com/hkloudou/lake/v2/internal/config"
	"github.com/hkloudou/lake/v2/internal/index"
	"github.com/hkloudou/lake/v2/internal/merge"
	"github.com/hkloudou/lake/v2/internal/storage"
	"github.com/hkloudou/lake/v2/internal/trace"
	"github.com/redis/go-redis/v9"
)

// Client is the main interface for Lake v2
type Client struct {
	rdb       *redis.Client
	writer    *index.Writer
	reader    *index.Reader
	merger    *merge.Engine // Legacy (deprecated)
	configMgr *config.Manager
	cache     cache.Cache

	// Lazy-loaded components
	mu      sync.RWMutex
	storage storage.Storage
	// snapMgr *snapshot.Manager
	config *config.Config
}

// Option is a function that configures the client
type Option struct {
	Storage       storage.Storage
	CacheProvider cache.Cache
}

// NewLake creates a new Lake client with the given Redis URL
// Config is loaded lazily on first operation
func NewLake(metaUrl string, opts ...func(*Option)) *Client {
	// Parse Redis URL
	redisOpt, err := redis.ParseURL(metaUrl)
	if err != nil {
		// Fallback to treating it as an address
		redisOpt = &redis.Options{
			Addr: metaUrl,
		}
	}

	rdb := redis.NewClient(redisOpt)

	// Apply options
	option := &Option{}
	for _, opt := range opts {
		opt(option)
	}

	writer := index.NewWriter(rdb)
	reader := index.NewReader(rdb)
	merger := merge.NewEngine()
	configMgr := config.NewManager(rdb)

	// Use provided cache or default to no-op cache
	cacheProvider := option.CacheProvider
	if cacheProvider == nil {
		cacheProvider = cache.NewNoOpCache()
	}

	client := &Client{
		rdb:       rdb,
		writer:    writer,
		reader:    reader,
		merger:    merger,
		configMgr: configMgr,
		storage:   option.Storage, // May be nil, will be loaded lazily
		cache:     cacheProvider,
	}

	return client
}

// WithCache returns an option function that sets the cache provider
func WithCache(cacheProvider cache.Cache) func(*Option) {
	return func(opt *Option) {
		opt.CacheProvider = cacheProvider
	}
}

func WithRedisCache(metaUrl string, ttl time.Duration) func(*Option) {
	return func(opt *Option) {
		cacheProvider, err := cache.NewRedisCacheWithURL(metaUrl, ttl)
		if err != nil {
			panic(err)
		}
		opt.CacheProvider = cacheProvider
	}
}

// WithStorage returns an option function that sets the storage provider
func WithStorage(storage storage.Storage) func(*Option) {
	return func(opt *Option) {
		opt.Storage = storage
	}
}

// ensureInitialized ensures storage and snapMgr are initialized
// Loads config from Redis if not already loaded
func (c *Client) ensureInitialized(ctx context.Context) error {
	c.mu.RLock()
	if c.storage != nil {
		c.mu.RUnlock()
		return nil
	}
	c.mu.RUnlock()

	c.mu.Lock()
	defer c.mu.Unlock()

	// Double-check after acquiring write lock
	if c.storage != nil {
		return nil
	}

	// Load config and initialize storage if not provided
	if c.storage == nil {
		// Load config from Redis if not already loaded
		if c.config == nil {
			cfg, err := c.configMgr.Load(ctx)
			if err != nil {
				return fmt.Errorf("failed to load config from Redis (lake.setting): %w", err)
			}
			c.config = cfg
		}

		// Create storage from config - must succeed, no fallback
		stor, err := c.config.CreateStorage()
		if err != nil {
			return fmt.Errorf("failed to create %s storage: %w", c.config.Storage, err)
		}
		c.storage = stor

		// Set index prefix based on config: Storage:Name
	}
	prefix := c.storage.RedisPrefix()
	c.writer.SetPrefix(prefix)
	c.reader.SetPrefix(prefix)

	// Initialize snapshot manager
	// if c.snapMgr == nil {
	// 	c.snapMgr = snapshot.NewManager(c.storage, c.reader, c.writer)
	// }

	return nil
}

// WriteRequest represents a write request
type WriteRequest struct {
	Catalog   string    // Catalog name
	Field     string    // JSON path (e.g., "user.profile.name")
	Body      []byte    // JSON body to write (raw bytes from network)
	MergeType MergeType // Merge strategy (Replace, RFC7396, or RFC6902)
}

// WriteResult represents the write result
type WriteResult struct {
	TsSeqID   string // Generated timestamp_seqid
	Timestamp int64  // Unix timestamp
	SeqID     int64  // Sequence ID
}

// Write writes data to the catalog
// Timestamp and sequence ID are auto-generated by Redis
//
// Merge Types:
// - MergeTypeReplace (0): Simple field replacement
// - MergeTypeRFC7396 (1): RFC 7396 JSON Merge Patch (field-level or root-level)
// - MergeTypeRFC6902 (2): RFC 6902 JSON Patch (field-level or root-level)
//
// Field parameter:
// - For Replace/RFC7396: specifies the target field (empty "" means root document)
// - For RFC6902: empty "" means operations apply to root, non-empty means operations scope to that field
//
// Body parameter:
// - Raw JSON bytes (typically from HTTP request body)
// - No additional marshaling/unmarshaling overhead
// - Examples:
//   - String value: []byte(`"Alice"`)
//   - Number value: []byte(`30`)
//   - Object value: []byte(`{"age":30}`)
//   - RFC7396 patch: []byte(`{"age":31,"city":null}`)
//   - RFC6902 patch: []byte(`[{"op":"add","path":"/a","value":1}]`)
func (c *Client) Write(ctx context.Context, req WriteRequest) (*WriteResult, error) {
	tr := trace.FromContext(ctx)

	if req.MergeType == 0 {
		return nil, fmt.Errorf("merge type unknown is not supported")
	}
	if len(req.Body) == 0 {
		return nil, fmt.Errorf("body is empty")
	}

	// Ensure initialized before operation
	if err := c.ensureInitialized(ctx); err != nil {
		return nil, err
	}
	tr.RecordSpan("Write.Init")

	// Step 1: Atomically get TimeSeqID and pre-commit to Redis (pending state)
	tsSeq, pendingMember, err := c.writer.GetTimeSeqIDAndPreCommit(ctx, req.Catalog, req.Field, req.MergeType)
	if err != nil {
		return nil, fmt.Errorf("failed to generate timeseq and precommit: %w", err)
	}
	tr.RecordSpan("Write.PreCommit", map[string]any{
		"tsSeq":  tsSeq.String(),
		"seqID":  tsSeq.SeqID,
		"member": pendingMember,
	})

	// Step 2: Write to storage
	if c.storage == nil {
		return nil, fmt.Errorf("storage not initialized")
	}
	storageKey := c.storage.MakeDeltaKey(req.Catalog, tsSeq, int(req.MergeType))
	if err := c.storage.Put(ctx, storageKey, req.Body); err != nil {
		// Rollback: remove pending member from Redis
		// catalogKey := c.writer.MakeCatalogKey(req.Catalog)
		// c.rdb.ZRem(ctx, catalogKey, pendingMember)
		tr.RecordSpan("Write.Rollback")
		return nil, fmt.Errorf("failed to write to storage: %w", err)
	}
	tr.RecordSpan("Write.StoragePut", map[string]any{
		"key":  storageKey,
		"size": len(req.Body),
	})

	// Step 3: Atomically commit (remove pending, add committed)
	committedMember := index.EncodeDeltaMember(req.Field, tsSeq.String(), req.MergeType)
	err = c.writer.Commit(ctx, req.Catalog, pendingMember, committedMember, tsSeq.Score())
	if err != nil {
		return nil, fmt.Errorf("failed to commit: %w", err)
	}
	tr.RecordSpan("Write.Commit")

	return &WriteResult{
		TsSeqID:   tsSeq.String(),
		Timestamp: tsSeq.Timestamp,
		SeqID:     tsSeq.SeqID,
	}, nil
}

func (c *Client) readData(ctx context.Context, list *ListResult) ([]byte, error) {
	tr := trace.FromContext(ctx)

	// Check for pending writes error from List
	if list.Err != nil {
		return nil, list.Err
	}

	// If pending writes detected, return error
	if list.HasPending {
		return nil, fmt.Errorf("pending writes detected: %w", list.Err)
	}

	// Ensure initialized before operation
	if err := c.ensureInitialized(ctx); err != nil {
		return nil, err
	}
	tr.RecordSpan("Read.Init")

	// Parallel execution: load snapshot base data and delta bodies concurrently
	var baseData []byte
	var baseDataErr error
	var deltasErr error

	var wg sync.WaitGroup

	// Goroutine 1: Load snapshot base data from cache/storage
	wg.Add(1)
	go func() {
		defer wg.Done()
		if list.LatestSnap != nil {
			key := c.storage.MakeSnapKey(list.catalog, list.LatestSnap.StartTsSeq, list.LatestSnap.StopTsSeq)
			namespace := c.storage.RedisPrefix()

			// Use cache to load snapshot data with namespace
			baseData, baseDataErr = c.cache.Take(namespace, key, func() ([]byte, error) {
				// Cache miss: load from storage
				return c.storage.Get(ctx, key)
			})
		} else {
			baseData = []byte("{}")
		}
	}()

	// Goroutine 2: Load delta bodies concurrently (max 10 workers)
	wg.Add(1)
	go func() {
		defer wg.Done()
		deltasErr = c.fillDeltasBody(ctx, list.catalog, list.Entries)
	}()

	// Wait for both operations to complete
	wg.Wait()

	// Check for errors
	if baseDataErr != nil {
		return nil, fmt.Errorf("failed to load snapshot: %w", baseDataErr)
	}
	if deltasErr != nil {
		return nil, fmt.Errorf("failed to load deltas: %w", deltasErr)
	}

	tr.RecordSpan("Read.LoadData")

	// Merge entries with base data (pure CPU operation, all data loaded)
	resultData, err := c.merger.Merge(list.catalog, baseData, list.Entries)
	if err != nil {
		return nil, err
	}
	tr.RecordSpan("Read.Merge")

	// Generate and save new snapshot if there are new entries
	if nextSnap := list.NextSnap(); nextSnap != nil {
		_, err = c.saveSnapshot(ctx, list.catalog, nextSnap.StartTsSeq, nextSnap.StopTsSeq, resultData)
		if err != nil {
			// Snapshot save failure should not fail the read
			// Record in trace for debugging
			tr.RecordSpan("Read.SnapshotSave", map[string]interface{}{
				"success": false,
				"error":   err.Error(),
			})
		} else {
			tr.RecordSpan("Read.SnapshotSave", map[string]interface{}{
				"success": true,
			})
		}
	}

	return resultData, nil
}

// fillDeltasBody fills the Body field for all deltas concurrently
// Uses a worker pool with max 10 concurrent goroutines
// Idempotent: skips deltas that already have Body loaded (len(Body) > 0)
// Returns error immediately if any delta fails to load (no partial success)
func (c *Client) fillDeltasBody(ctx context.Context, catalog string, deltas []index.DeltaInfo) error {
	if len(deltas) == 0 {
		return nil
	}

	// Channel for work distribution
	type job struct {
		index int
		delta *index.DeltaInfo
	}

	jobs := make(chan job, len(deltas))
	done := make(chan error, 1) // Buffered channel for first error

	// Count deltas that need loading
	needLoading := 0
	for i := range deltas {
		if len(deltas[i].Body) == 0 {
			needLoading++
		}
	}

	if needLoading == 0 {
		return nil // All bodies already loaded
	}

	// Worker pool with max 10 concurrent workers
	maxWorkers := 10
	if needLoading < maxWorkers {
		maxWorkers = needLoading
	}

	// Context for early cancellation on error
	workerCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Start workers
	var wg sync.WaitGroup
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for j := range jobs {
				// Check if context cancelled (another worker failed)
				select {
				case <-workerCtx.Done():
					return
				default:
				}

				// Skip if already loaded
				if len(j.delta.Body) > 0 {
					continue
				}

				key := c.storage.MakeDeltaKey(catalog, j.delta.TsSeq, int(j.delta.MergeType))
				data, err := c.storage.Get(workerCtx, key)
				if err != nil {
					// Send error and cancel other workers
					select {
					case done <- fmt.Errorf("failed to load delta %d (%s): %w", j.index, j.delta.TsSeq, err):
					default:
					}
					cancel()
					return
				}
				j.delta.Body = data
			}
		}()
	}

	// Send jobs in a separate goroutine
	go func() {
		for i := range deltas {
			select {
			case <-workerCtx.Done():
				return
			case jobs <- job{index: i, delta: &deltas[i]}:
			}
		}
		close(jobs)
	}()

	// Wait for all workers or first error
	go func() {
		wg.Wait()
		close(done)
	}()

	// Return first error or nil
	if err := <-done; err != nil {
		return err
	}

	return nil
}
