package lake

import (
	"context"
	"fmt"
	"sync"
	"time"

	"github.com/hkloudou/lake/v2/internal/cache"
	"github.com/hkloudou/lake/v2/internal/config"
	"github.com/hkloudou/lake/v2/internal/index"
	"github.com/hkloudou/lake/v2/internal/merge"
	"github.com/hkloudou/lake/v2/internal/snapshot"
	"github.com/hkloudou/lake/v2/internal/storage"
	"github.com/redis/go-redis/v9"
	"github.com/tidwall/gjson"
	"github.com/tidwall/sjson"
)

// Global merger instances (stateless, safe to share)
var (
	rfc7396Merger = merge.NewRFC7396Merger()
	rfc6902Merger = merge.NewRFC6902Merger()
)

// Client is the main interface for Lake v2
type Client struct {
	rdb       *redis.Client
	writer    *index.Writer
	reader    *index.Reader
	merger    *merge.Engine // Legacy (deprecated)
	configMgr *config.Manager
	cache     cache.Cache

	// Lazy-loaded components
	mu      sync.RWMutex
	storage storage.Storage
	snapMgr *snapshot.Manager
	config  *config.Config
}

// Option is a function that configures the client
type Option struct {
	Storage       storage.Storage
	CacheProvider cache.Cache
}

// NewLake creates a new Lake client with the given Redis URL
// Config is loaded lazily on first operation
func NewLake(metaUrl string, opts ...func(*Option)) *Client {
	// Parse Redis URL
	redisOpt, err := redis.ParseURL(metaUrl)
	if err != nil {
		// Fallback to treating it as an address
		redisOpt = &redis.Options{
			Addr: metaUrl,
		}
	}

	rdb := redis.NewClient(redisOpt)

	// Apply options
	option := &Option{}
	for _, opt := range opts {
		opt(option)
	}

	writer := index.NewWriter(rdb)
	reader := index.NewReader(rdb)
	merger := merge.NewEngine()
	configMgr := config.NewManager(rdb)

	// Use provided cache or default to no-op cache
	cacheProvider := option.CacheProvider
	if cacheProvider == nil {
		cacheProvider = cache.NewNoOpCache()
	}

	client := &Client{
		rdb:       rdb,
		writer:    writer,
		reader:    reader,
		merger:    merger,
		configMgr: configMgr,
		storage:   option.Storage, // May be nil, will be loaded lazily
		cache:     cacheProvider,
	}

	return client
}

// WithCache returns an option function that sets the cache provider
func WithCache(cacheProvider cache.Cache) func(*Option) {
	return func(opt *Option) {
		opt.CacheProvider = cacheProvider
	}
}

func WithRedisCache(metaUrl string, ttl time.Duration) func(*Option) {
	return func(opt *Option) {
		cacheProvider, err := cache.NewRedisCacheWithURL(metaUrl, ttl)
		if err != nil {
			panic(err)
		}
		opt.CacheProvider = cacheProvider
	}
}

// WithStorage returns an option function that sets the storage provider
func WithStorage(storage storage.Storage) func(*Option) {
	return func(opt *Option) {
		opt.Storage = storage
	}
}

// ensureInitialized ensures storage and snapMgr are initialized
// Loads config from Redis if not already loaded
func (c *Client) ensureInitialized(ctx context.Context) error {
	c.mu.RLock()
	if c.storage != nil && c.snapMgr != nil {
		c.mu.RUnlock()
		return nil
	}
	c.mu.RUnlock()

	c.mu.Lock()
	defer c.mu.Unlock()

	// Double-check after acquiring write lock
	if c.storage != nil && c.snapMgr != nil {
		return nil
	}

	// Load config and initialize storage if not provided
	if c.storage == nil {
		// Load config from Redis if not already loaded
		if c.config == nil {
			cfg, err := c.configMgr.Load(ctx)
			if err != nil {
				return fmt.Errorf("failed to load config from Redis (lake.setting): %w", err)
			}
			c.config = cfg
		}

		// Create storage from config - must succeed, no fallback
		stor, err := c.config.CreateStorage()
		if err != nil {
			return fmt.Errorf("failed to create %s storage: %w", c.config.Storage, err)
		}
		c.storage = stor

		// Set index prefix based on config: Storage:Name
	}
	prefix := c.storage.RedisPrefix()
	c.writer.SetPrefix(prefix)
	c.reader.SetPrefix(prefix)

	// Initialize snapshot manager
	if c.snapMgr == nil {
		c.snapMgr = snapshot.NewManager(c.storage, c.reader, c.writer)
	}

	return nil
}

// WriteRequest represents a write request
type WriteRequest struct {
	Catalog   string          // Catalog name
	Field     string          // JSON path (e.g., "user.profile.name")
	Body      []byte          // JSON body to write (raw bytes from network)
	MergeType index.MergeType // Merge strategy (Replace or Merge)
}

// WriteResult represents the write result
type WriteResult struct {
	TsSeqID   string // Generated timestamp_seqid
	Timestamp int64  // Unix timestamp
	SeqID     int64  // Sequence ID
}

// Write writes data to the catalog
// Timestamp and sequence ID are auto-generated by Redis
//
// Merge Types:
// - MergeTypeReplace (0): Simple field replacement
// - MergeTypeRFC7396 (1): RFC 7396 JSON Merge Patch (field-level or root-level)
// - MergeTypeRFC6902 (2): RFC 6902 JSON Patch (field-level or root-level)
//
// Field parameter:
// - For Replace/RFC7396: specifies the target field (empty "" means root document)
// - For RFC6902: empty "" means operations apply to root, non-empty means operations scope to that field
//
// Body parameter:
// - Raw JSON bytes (typically from HTTP request body)
// - No additional marshaling/unmarshaling overhead
// - Examples:
//   - String value: []byte(`"Alice"`)
//   - Number value: []byte(`30`)
//   - Object value: []byte(`{"age":30}`)
//   - RFC7396 patch: []byte(`{"age":31,"city":null}`)
//   - RFC6902 patch: []byte(`[{"op":"add","path":"/a","value":1}]`)
func (c *Client) Write(ctx context.Context, req WriteRequest) (*WriteResult, error) {
	if req.MergeType == 0 {
		return nil, fmt.Errorf("merge type replace is not supported")
	}
	if len(req.Body) == 0 {
		return nil, fmt.Errorf("body is empty")
	}
	// Ensure initialized before operation
	if err := c.ensureInitialized(ctx); err != nil {
		return nil, err
	}

	tsSeq, err := c.writer.GetTimeSeqID(ctx, req.Catalog)
	if err != nil {
		return nil, fmt.Errorf("failed to generate time+seqid: %w", err)
	}
	if tsSeq.SeqID > 1000000 {
		return nil, fmt.Errorf("seqid too large: %d", tsSeq.SeqID)
	}

	// Write to storage with filename: catalog/{ts}_{seqid}_{mergetype}.json
	if c.storage == nil {
		return nil, fmt.Errorf("storage not initialized")
	}
	key := storage.MakeDeltaKey(req.Catalog, tsSeq, int(req.MergeType))
	if err := c.storage.Put(ctx, key, req.Body); err != nil {
		// Rollback: remove from Redis index (best effort)
		// TODO: implement proper rollback mechanism
		return nil, fmt.Errorf("failed to write to storage: %w", err)
	}
	// Generate time+seqid and add to index (atomically)
	err = c.writer.AddWithTimeSeq(ctx, tsSeq, req.Catalog, req.Field, req.MergeType)
	if err != nil {
		return nil, fmt.Errorf("failed to add to index: %w", err)
	}

	return &WriteResult{
		TsSeqID:   tsSeq.String(),
		Timestamp: tsSeq.Timestamp,
		SeqID:     tsSeq.SeqID,
	}, nil
}

func (c *Client) readData(ctx context.Context, list *ListResult) ([]byte, error) {
	// Ensure initialized before operation
	if err := c.ensureInitialized(ctx); err != nil {
		return nil, err
	}

	// Parallel execution: load snapshot base data and delta bodies concurrently
	var baseData []byte
	var baseDataErr error
	var deltasErr error

	var wg sync.WaitGroup

	// Goroutine 1: Load snapshot base data from cache/storage
	wg.Add(1)
	go func() {
		defer wg.Done()
		if list.LatestSnap != nil {
			key := storage.MakeSnapKey(list.catalog, list.LatestSnap.StartTsSeq, list.LatestSnap.StopTsSeq)
			namespace := c.storage.RedisPrefix()

			// Use cache to load snapshot data with namespace
			baseData, baseDataErr = c.cache.Take(namespace, key, func() ([]byte, error) {
				// Cache miss: load from storage
				return c.storage.Get(ctx, key)
			})
		} else {
			baseData = []byte("{}")
		}
	}()

	// Goroutine 2: Load delta bodies concurrently (max 10 workers)
	wg.Add(1)
	go func() {
		defer wg.Done()
		deltasErr = c.fillDeltasBody(ctx, list.catalog, list.Entries)
	}()

	// Wait for both operations to complete
	wg.Wait()

	// Check for errors
	if baseDataErr != nil {
		return nil, fmt.Errorf("failed to load snapshot: %w", baseDataErr)
	}
	if deltasErr != nil {
		return nil, fmt.Errorf("failed to load deltas: %w", deltasErr)
	}

	// Merge entries with base data (pure CPU operation, all data loaded)
	resultData, err := c.mergeEntries(ctx, list.catalog, baseData, list.Entries)
	if err != nil {
		return nil, err
	}

	// Generate and save new snapshot if there are new entries
	if len(list.Entries) > 0 {
		nextSnap := list.NextSnap()
		_, err = list.client.snapMgr.Save(ctx, list.catalog, nextSnap.StartTsSeq, nextSnap.StopTsSeq, resultData)
		if err != nil {
			return nil, fmt.Errorf("failed to save snapshot: %w", err)
		}
	}

	return resultData, nil
}

// fillDeltasBody fills the Body field for all deltas concurrently
// Uses a worker pool with max 10 concurrent goroutines
// Idempotent: skips deltas that already have Body loaded (len(Body) > 0)
// Returns error immediately if any delta fails to load (no partial success)
func (c *Client) fillDeltasBody(ctx context.Context, catalog string, deltas []index.DeltaInfo) error {
	if len(deltas) == 0 {
		return nil
	}

	// Channel for work distribution
	type job struct {
		index int
		delta *index.DeltaInfo
	}

	jobs := make(chan job, len(deltas))
	done := make(chan error, 1) // Buffered channel for first error

	// Count deltas that need loading
	needLoading := 0
	for i := range deltas {
		if len(deltas[i].Body) == 0 {
			needLoading++
		}
	}

	if needLoading == 0 {
		return nil // All bodies already loaded
	}

	// Worker pool with max 10 concurrent workers
	maxWorkers := 10
	if needLoading < maxWorkers {
		maxWorkers = needLoading
	}

	// Context for early cancellation on error
	workerCtx, cancel := context.WithCancel(ctx)
	defer cancel()

	// Start workers
	var wg sync.WaitGroup
	for i := 0; i < maxWorkers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for j := range jobs {
				// Check if context cancelled (another worker failed)
				select {
				case <-workerCtx.Done():
					return
				default:
				}

				// Skip if already loaded
				if len(j.delta.Body) > 0 {
					continue
				}

				key := storage.MakeDeltaKey(catalog, j.delta.TsSeq, int(j.delta.MergeType))
				data, err := c.storage.Get(workerCtx, key)
				if err != nil {
					// Send error and cancel other workers
					select {
					case done <- fmt.Errorf("failed to load delta %d (%s): %w", j.index, j.delta.TsSeq, err):
					default:
					}
					cancel()
					return
				}
				j.delta.Body = data
			}
		}()
	}

	// Send jobs in a separate goroutine
	go func() {
		for i := range deltas {
			select {
			case <-workerCtx.Done():
				return
			case jobs <- job{index: i, delta: &deltas[i]}:
			}
		}
		close(jobs)
	}()

	// Wait for all workers or first error
	go func() {
		wg.Wait()
		close(done)
	}()

	// Return first error or nil
	if err := <-done; err != nil {
		return err
	}

	return nil
}

// ReadRequest represents a read request
// type ReadRequest struct {
// 	Catalog      string // Catalog name
// 	GenerateSnap bool   // Whether to generate snapshot automatically
// }

// mergeEntries merges entries into baseData
// This is the single source of truth for data merging
// Assumes all entry.Body fields are already loaded (by fillDeltasBody)
func (c *Client) mergeEntries(ctx context.Context, catalog string, baseData []byte, entries []index.DeltaInfo) ([]byte, error) {
	merged := baseData
	for _, entry := range entries {
		// Use pre-loaded Body data (filled by fillDeltasBody)
		if len(entry.Body) == 0 {
			continue // Skip entries without body data
		}

		data := entry.Body
		var err error

		// Merge using the strategy from entry
		switch entry.MergeType {
		case index.MergeTypeReplace:
			// Simple replace: set the field value directly
			merged, err = sjson.SetRawBytes(merged, entry.Field, data)
			if err != nil {
				return nil, fmt.Errorf("failed to set field: %w", err)
			}

		case index.MergeTypeRFC7396:
			// RFC 7396 JSON Merge Patch
			// https://datatracker.ietf.org/doc/html/rfc7396
			// Applies patch to the field's value (local scope)
			fieldValue := gjson.GetBytes(merged, entry.Field).Raw
			if fieldValue == "" {
				fieldValue = "{}" // Default to empty object if field doesn't exist
			}

			mergedData, err := rfc7396Merger.Merge([]byte(fieldValue), data)
			if err != nil {
				return nil, fmt.Errorf("RFC7396 merge failed: %w", err)
			}

			// Set the merged result back to the field
			merged, err = sjson.SetRawBytes(merged, entry.Field, mergedData)
			if err != nil {
				return nil, fmt.Errorf("failed to set merged field: %w", err)
			}

		case index.MergeTypeRFC6902:
			// RFC 6902 JSON Patch
			// https://datatracker.ietf.org/doc/html/rfc6902
			// If field is empty, patches entire document; otherwise patches that field's value
			merged, err = rfc6902Merger.Merge(merged, data, entry.Field)
			if err != nil {
				return nil, fmt.Errorf("RFC6902 patch failed: %w", err)
			}

		default:
			return nil, fmt.Errorf("unknown merge type: %d", entry.MergeType)
		}
	}
	return merged, nil
}

// Read reads and merges data from the catalog
func (c *Client) List(ctx context.Context, catalog string) (*ListResult, error) {
	// Ensure initialized before operation
	if err := c.ensureInitialized(ctx); err != nil {
		return nil, err
	}

	// Try to get existing snapshot
	snap, err := c.reader.GetLatestSnap(ctx, catalog)
	if err != nil {
		return nil, fmt.Errorf("failed to get snapshot: %w", err)
	}

	var deltas []index.DeltaInfo

	if snap != nil {
		deltas, err = c.reader.ReadSince(ctx, catalog, snap.StopTsSeq.Score())
		if err != nil {
			return nil, fmt.Errorf("failed to read delta index: %w", err)
		}
	} else {
		// No snapshot, read all
		deltas, err = c.reader.ReadAll(ctx, catalog)
		if err != nil {
			return nil, fmt.Errorf("failed to read all delta index: %w", err)
		}
	}

	return &ListResult{
		client:     c,
		catalog:    catalog,
		LatestSnap: snap,
		Entries:    deltas,
	}, nil

	// Merge data from all entries (rebuild from scratch)
	// baseData := make(map[string]any)
	// entries := allEntries

	// Merge data (single source of truth)
	// merged, err := c.mergeEntries(ctx, catalog, snap, allEntries)
	// if err != nil {
	// 	return nil, err
	// }

	// result := &ReadResult{
	// 	Data:       merged,
	// 	LatestSnap: snap,
	// 	Entries:    allEntries, // Return all entries for debugging
	// }

	// Generate snapshot if requested
	// if req.GenerateSnap && len(allEntries) > 0 {
	// 	// Determine startTsSeq and stopTsSeq
	// 	var startTsSeq index.TimeSeqID
	// 	if snap != nil {
	// 		// Continue from previous snapshot
	// 		startTsSeq = snap.StopTsSeq
	// 	}

	// 	// Use the last entry's TsSeqID as stop point
	// 	lastEntry := allEntries[len(allEntries)-1]
	// 	// stopTsSeq, err := index.ParseTimeSeqID()
	// 	// if err != nil {
	// 	// 	return nil, fmt.Errorf("failed to parse last entry TsSeqID: %w", err)
	// 	// }

	// 	// score := tsSeq.Score()

	// 	// Save snapshot metadata (time range only, no data)
	// 	newSnap, err := c.snapMgr.Save(ctx, req.Catalog, startTsSeq, lastEntry.TsSeq, lastEntry.TsSeq.Score())
	// 	if err == nil {
	// 		result.LatestSnap = &index.SnapInfo{
	// 			StartTsSeq: newSnap.StartTsSeq,
	// 			StopTsSeq:  newSnap.StopTsSeq,
	// 			Score:      lastEntry.TsSeq.Score(),
	// 		}
	// 	}
	// }

	// return result, nil
}

// GetConfig returns the current config (loads from Redis if needed)
// DEPRECATED: Temporarily disabled. DO NOT DELETE this code.
// Will be re-enabled after API stabilization.
/*
func (c *Client) GetConfig(ctx context.Context) (*config.Config, error) {
	c.mu.RLock()
	if c.config != nil {
		cfg := c.config
		c.mu.RUnlock()
		return cfg, nil
	}
	c.mu.RUnlock()

	// Load config
	cfg, err := c.configMgr.Load(ctx)
	if err != nil {
		return nil, err
	}

	c.mu.Lock()
	c.config = cfg
	c.mu.Unlock()

	return cfg, nil
}
*/

// UpdateConfig updates the config in Redis
// DEPRECATED: Temporarily disabled. DO NOT DELETE this code.
// Will be re-enabled after storage reinitialization logic is implemented.
/*
func (c *Client) UpdateConfig(ctx context.Context, cfg *config.Config) error {
	if err := c.configMgr.Save(ctx, cfg); err != nil {
		return err
	}

	// Update cached config
	c.mu.Lock()
	c.config = cfg
	// TODO: Reinitialize storage based on new config
	c.mu.Unlock()

	return nil
}
*/
